{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb9608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating each neuron individually...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 2/155 [00:57<1:13:29, 28.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    113\u001b[39m labels = batch[\u001b[33m\"\u001b[39m\u001b[33mneural_activity\u001b[39m\u001b[33m\"\u001b[39m][:, neuron_id]  \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[32m    115\u001b[39m neuron_idx = torch.full((image_emb.size(\u001b[32m0\u001b[39m),), neuron_id, dtype=torch.long)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneuron_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m probs = torch.sigmoid(logits)\n\u001b[32m    119\u001b[39m preds = (probs > \u001b[32m0.5\u001b[39m).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mPixelAttentionModel.forward\u001b[39m\u001b[34m(self, image_embedding, neuron_idx)\u001b[39m\n\u001b[32m     75\u001b[39m x = combined.view(B_flat, F_feat, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B*K, F, 1)\u001b[39;00m\n\u001b[32m     77\u001b[39m Q = \u001b[38;5;28mself\u001b[39m.to_q(x)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m K_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m V = \u001b[38;5;28mself\u001b[39m.to_v(x)\n\u001b[32m     81\u001b[39m attn_scores = torch.matmul(Q, K_.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)) / (Q.shape[-\u001b[32m1\u001b[39m] ** \u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# === Load embeddings and events ===\n",
    "with open('/home/maria/Documents/HuggingMouseData/MouseViTEmbeddings/google_vit-base-patch16-224_embeddings_softmax.pkl', 'rb') as f:\n",
    "    image_embeddings_dict = pickle.load(f)\n",
    "\n",
    "image_embeddings = image_embeddings_dict['natural_scenes']  # (118, D)\n",
    "image_embeddings_repeated = np.repeat(image_embeddings, 50, axis=0)  # (5900, D)\n",
    "assert image_embeddings_repeated.shape[0] == 5900\n",
    "\n",
    "events = np.load('/home/maria/Documents/AllenBrainObservatory/neural_activity_matrices/500860585_neural_responses.npy')  # (N_neurons, 5900)\n",
    "\n",
    "# === Dataset ===\n",
    "class NeuronVisionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_embeddings, neural_events):\n",
    "        assert image_embeddings.shape[0] == neural_events.shape[1]\n",
    "        self.image_embeddings = torch.tensor(image_embeddings, dtype=torch.float32)\n",
    "        self.neural_events = torch.tensor(neural_events.T, dtype=torch.float32)  # (5900, N_neurons)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_embeddings.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"image_embedding\": self.image_embeddings[idx],   # (D,)\n",
    "            \"neural_activity\": self.neural_events[idx]       # (N_neurons,)\n",
    "        }\n",
    "\n",
    "full_dataset = NeuronVisionDataset(image_embeddings_repeated, events)\n",
    "full_loader = DataLoader(full_dataset, batch_size=4, shuffle=False)  # Small batch size\n",
    "\n",
    "# === Model ===\n",
    "class PixelAttentionModel(nn.Module):\n",
    "    def __init__(self, image_dim, neuron_dim, num_neurons, attention_dim=32):\n",
    "        super().__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.neuron_dim = neuron_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        self.feature_dim = image_dim + neuron_dim\n",
    "\n",
    "        self.neuron_embeddings = nn.Parameter(torch.randn(num_neurons, neuron_dim))\n",
    "        self.to_q = nn.Linear(1, attention_dim)\n",
    "        self.to_k = nn.Linear(1, attention_dim)\n",
    "        self.to_v = nn.Linear(1, attention_dim)\n",
    "        self.output_proj = nn.Linear(self.feature_dim * attention_dim, 1)\n",
    "\n",
    "    def forward(self, image_embedding, neuron_idx):\n",
    "        B, D = image_embedding.shape\n",
    "        neuron_idx = neuron_idx.to(torch.long)\n",
    "\n",
    "        if neuron_idx.ndim == 1:\n",
    "            K = 1\n",
    "            neuron_emb = self.neuron_embeddings[neuron_idx]  # (B, D')\n",
    "        elif neuron_idx.ndim == 2:\n",
    "            K = neuron_idx.shape[1]\n",
    "            neuron_emb = self.neuron_embeddings[neuron_idx]  # (B, K, D')\n",
    "        else:\n",
    "            raise ValueError(\"neuron_idx must be shape (B,) or (B, K)\")\n",
    "\n",
    "        if K == 1:\n",
    "            image_exp = image_embedding  # (B, D)\n",
    "        else:\n",
    "            image_exp = image_embedding.unsqueeze(1).expand(-1, K, -1)  # (B, K, D)\n",
    "\n",
    "        combined = torch.cat([image_exp, neuron_emb], dim=-1)  # (B, K, D + D')\n",
    "        B_flat = B * K if K > 1 else B\n",
    "        F_feat = self.feature_dim\n",
    "        x = combined.view(B_flat, F_feat, 1)  # (B*K, F, 1)\n",
    "\n",
    "        Q = self.to_q(x)\n",
    "        K_ = self.to_k(x)\n",
    "        V = self.to_v(x)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K_.transpose(-2, -1)) / (Q.shape[-1] ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_out = torch.matmul(attn_weights, V)  # (B*K, F, A)\n",
    "\n",
    "        attn_out_flat = attn_out.view(B_flat, -1)\n",
    "        output = self.output_proj(attn_out_flat).squeeze(-1)\n",
    "        return output.view(B, K) if K > 1 else output.view(B)\n",
    "\n",
    "# === Load model ===\n",
    "image_dim = image_embeddings_repeated.shape[1]\n",
    "num_neurons = events.shape[0]\n",
    "neuron_dim = 32\n",
    "attention_dim = 16\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = PixelAttentionModel(image_dim, neuron_dim, num_neurons, attention_dim)\n",
    "model.load_state_dict(torch.load(\"/home/maria/LuckyMouse/notebooks/models/fold_1.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Per-neuron evaluation loop ===\n",
    "per_neuron_correct = torch.zeros(num_neurons)\n",
    "per_neuron_total = torch.zeros(num_neurons)\n",
    "\n",
    "print(\"üîç Evaluating each neuron individually...\")\n",
    "with torch.no_grad():\n",
    "    for neuron_id in tqdm(range(num_neurons)):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in full_loader:\n",
    "            image_emb = batch[\"image_embedding\"]  # (B, D)\n",
    "            labels = batch[\"neural_activity\"][:, neuron_id]  # (B,)\n",
    "\n",
    "            neuron_idx = torch.full((image_emb.size(0),), neuron_id, dtype=torch.long)\n",
    "\n",
    "            logits = model(image_emb, neuron_idx)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "        per_neuron_correct[neuron_id] = correct\n",
    "        per_neuron_total[neuron_id] = total\n",
    "\n",
    "# === Report results ===\n",
    "overall_accuracy = per_neuron_correct.sum().item() / per_neuron_total.sum().item()\n",
    "per_neuron_accuracy = (per_neuron_correct / per_neuron_total).numpy()\n",
    "\n",
    "print(f\"\\n‚úÖ Overall accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "for i, acc in enumerate(per_neuron_accuracy):\n",
    "    print(f\"Neuron {i:3d} accuracy: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global 3.10)",
   "language": "python",
   "name": "global-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
